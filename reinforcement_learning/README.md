## Reinforcement Learning

| Paper                                                                                                                                                                                                              | Notes                                                         | Author                                        | Summary                                                                                                                                                                                                                                 |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------:|:---------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| [DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION](https://arxiv.org/pdf/1912.01603.pdf) (ICLR '20)                                                                                                         | [HackMD](https://hackmd.io/@iGBkTz2JQ2eBRM83nuhCuA/Hk9dpK0vd)            | [Raj](https://github.com/RajGhugare19)        |This paper focuses to learn long-horizon behaviors by propagating analytic value gradients through imagined trajectories using a recurrent state space model (PlaNet, haffner et al)                                                                               |
| [The Value Equivalence Principle for Model-Based Reinforcement Learning](https://arxiv.org/abs/2011.03506) (NeurIPS '20)                                                                                                         | [HackMD](https://hackmd.io/@Raj-Ghugare/HkEY6o9MP)            | [Raj](https://github.com/RajGhugare19)        |This paper introduces and studies the concept of equivalence for Reinforcement Learning models with respect to a set of policies and value functions. It further shows that this principle can be leveraged to find models constrained by representational capacity, which are better than their maximum likelihood counterparts.                                                                                 |
| [Stackelberg Actor-critic: A game theoretic perspective](https://hackmd.io/@FtbpSED3RQWclbmbmkChEA/rJFUQA1QO)                                                                                                      | [HackMD](https://hackmd.io/@FtbpSED3RQWclbmbmkChEA/rJFUQA1QO)            | [Sharath](https://sharathraparthy.github.io/)        | This paper formulates the interaction between the actor and critic ans a stackelberg games and leverages the implicit function theorem to calculate the accurate gradient updates for actor and critic.                                                                                                                                                        |
| [Curriculum learning for Reinforcement Learning Domains](https://arxiv.org/abs/2003.04960)                                                                                                      | [HackMD](https://hackmd.io/@FtbpSED3RQWclbmbmkChEA/Sy0IVj8Ju)            | [Sharath](https://sharathraparthy.github.io/)        | This is a survey paper on curriculum learning methods in reinforcement learning.                                                                                                                                                       |
| [Policy Gradient Methods for Reinforcement Learning with Function Approximation](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) (NIPS 1999) | [HackMD](https://hackmd.io/@Raj-Ghugare/BJGFOdmCL)            | [Raj](https://github.com/RajGhugare19)        | This paper provides the first policy gradient algorithm based on neural networks.                                                                                                                                                       |
| [Reinforcement Learning via Fenchel Rockafellar Duality](https://arxiv.org/abs/2001.01866)                                                                                                                         | [HackMD](https://hackmd.io/@FtbpSED3RQWclbmbmkChEA/rkZ5s2Y1P) | [Sharath](https://sharathraparthy.github.io/) | This  paper reviews the basic concepts of fenchel duality, f-divergences and shows how can these set of tools can be applied tin the context of reinforcement learning to derive theoritcally as well as practically robust algorithms. |
| [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)                                                                                                     | [HackMD](https://hackmd.io/3azkwbmgRLSrqyvUHf5SqQ?view)       | [Raj](https://github.com/RajGhugare19)        | This paper gives an algorithm with an advantage estimator and TRPO technique to empirically guarantee monotonic policy improvement.                                                                                                     |
| [Off-Policy Actor-Critic](https://arxiv.org/abs/1205.4839) (ICML '12)                                                                                                                                              | [HackMD](https://hackmd.io/@FtbpSED3RQWclbmbmkChEA/BkcB-xwvI) | [Sharath](https://sharathraparthy.github.io/) | This paper presents the first off-policy version of the actor-critic algorithms and derives a simple and elegant algorithm which performs better than the existing algorithms on standard reinforcement-learning benchmark problems.    |
| [Combining Physical Simulators and Object-Based Networks for Control](https://arxiv.org/pdf/1904.06580.pdf) (ICRA '19)                                                                                             | [HackMD](https://hackmd.io/@FtbpSED3RQWclbmbmkChEA/Sy6GPG9MB) | [Sharath](https://sharathraparthy.github.io/) | In this paper the authors proposed a hybrid dynamics model, Simulation-Augemented Interaction Networks, where they incorporated Interaction Networks into a physics engine for solving real world complex robotics control tasks.       |
| [Learning Agile and Dynamic Motor Skills for Legged Robots](https://arxiv.org/abs/1901.08652)                                                                                                                      | [HackMD](https://hackmd.io/@FtbpSED3RQWclbmbmkChEA/ByzYzEhVS) | [Sharath](https://sharathraparthy.github.io/) | This paper tackles the sim2real transfer problem for legged robots.                                                                                                                                                                     |
| [PAC-Bounds-for-Multi-armed-Bandit](https://link.springer.com/chapter/10.1007/3-540-45435-7_18) (CoLT '02)                                                                                                         | [HackMD](https://hackmd.io/saK7DdqCRnyBfN3HykLhlA)            | [Raj](https://github.com/RajGhugare19)        | This paper provides a technique to guarantee PAC bounds based on the rewards distirbution of the particular problem achieving better sample complexity.                                                                                 |
|[Deep Reinforcement Learning for Dialogue Generation](https://arxiv.org/abs/1606.01541)|[HackMD](https://hackmd.io/@HnlvODbMQIiAlpHchdZpDQ/Sy4VbzgAt)|[Om](https://github.com/DigZator)| This paper discusses how better dialogue generation can be achieved using RL. It provides a technique to convert converstational properties like informativity, coherence and ease of answering into reward functions.|
|[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/pdf/1710.02298.pdf)|[HackMD](https://hackmd.io/@HnlvODbMQIiAlpHchdZpDQ/BkYl3IkaK)|[Om](https://github.com/DigZator)| The paper discusses add-ons to the DQN and A3C that can improve their performance, namely Double DQN, Prioritized Experience Replay, Dueling Network Architecture, Distributional Q-Learning, Noisy DQN. |
| [The Option-Critic Architecture](https://arxiv.org/abs/1609.05140) | [HackMD](https://hackmd.io/@HnlvODbMQIiAlpHchdZpDQ/SyI7nv7_q) | [Om](https://github.com/DigZator) | Paper discusses the hierarchical reinforcement learning method implimentation based on temporal abstractions. | 
| [Addressing Distribution Shift in Online Reinforcement Learning with Offline Datasets](https://offline-rl-neurips.github.io/pdf/13.pdf) | [HackMD](https://hackmd.io/@HnlvODbMQIiAlpHchdZpDQ/rkxHo6LL5) | [Om](https://github.com/DigZator) | The paper suggests and provides experimental justification for methods to tackle Distribution Shift. |
| [FeUdal Networks for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1703.01161) | [HackMD](https://hackmd.io/@HnlvODbMQIiAlpHchdZpDQ/HJoIiDw_c) | [Om](https://github.com/DigZator) | This paper describes the FeUdal Network model. Employs a manager-worker hierarchy. |